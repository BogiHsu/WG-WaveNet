<html><head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Audio samples from "WG-WaveNet: Real-Time High-Fidelity Speech Synthesis without GPU"</title>
    <link rel="stylesheet" type="text/css" href="index_files/stylesheet.css">
    <link rel="shortcut icon" href="icon.png">
  </head>
  <body>
    <article>
      <header>
        <h1>WG-WaveNet: Real-Time High-Fidelity Speech Synthesis without GPU</h1>
      </header>
    </article>

    <div><b>Paper:</b> <a href="https://arxiv.org">arXiv</a></div>
    <div><b>Authors:</b> Po-chun Hsu, Hung-yi Lee</div>
    <div><b>Abstract:</b> In this paper, we propose WG-WaveNet, a fast, lightweight, and high-quality waveform generation model. WG-WaveNet is composed of a compact flow-based model and a post-filter. The two components are jointly trained by maximizing the likelihood of the training data and optimizing loss functions on the frequency domains. As we design a flow-based model that is heavily compressed, the proposed model requires much less computational resources compared to other waveform generation models during both training and inference time; even though the model is highly compressed, the post-filter maintains the quality of generated waveform.
    Our PyTorch implementation can be trained using less than 8 GB GPU memory and generates audio samples at a rate of more than 5000 kHz on an NVIDIA 1080Ti GPU. Furthermore, even if synthesizing on a CPU, we show that the proposed method is capable of generating 44.1 kHz speech waveform 1.2 times faster than real-time. Experiments also show that the quality of generated audio is comparable to those of other methods. Audio samples are publicly available online.
    </div>

</body></html>
